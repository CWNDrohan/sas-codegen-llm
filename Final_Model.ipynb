{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install evaluate\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "AZ0Kh9lMYa5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kYqhu2pYIfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# PyTorch and Transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "\n",
        "# Dataset handling\n",
        "from datasets import Dataset\n",
        "\n",
        "# Parameter-efficient fine-tuning (LoRA)\n",
        "from peft import get_peft_model\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Visualization\n",
        "%matplotlib inline\n",
        "\n",
        "# Google Colab utilities\n",
        "from google.colab import drive\n",
        "\n",
        "# Evaluation metrics\n",
        "from evaluate import load\n",
        "\n",
        "# W&B for logging\n",
        "import wandb\n",
        "from trl import SFTTrainer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Step 1: Mount Google Drive and Define Data Path\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define parameters\n",
        "model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "max_seq_length = 4096\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 25\n",
        "batch_size = 16\n",
        "gradient_accumulation_steps = 8\n",
        "warmup_steps = 10\n",
        "output_dir = \"/content/drive/My Drive/School/UMBC/DATA690LLM/Results/Final\"\n",
        "data_dir = \"/content/drive/My Drive/School/UMBC/DATA690LLM/spec_code_file/\"\n",
        "lr_scheduler_type = \"cosine\"  # Choose between \"cosine\" or \"linear\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file into a pandas DataFrame\n",
        "json_path = f\"{data_dir}data_pairs_gptgen.json\"\n",
        "data = pd.read_json(json_path)\n",
        "\n",
        "# Display the first 10 observations without truncation\n",
        "pd.set_option('display.max_colwidth', None)  # Show full content of columns\n",
        "print(data.head(10))"
      ],
      "metadata": {
        "id": "G-UrNrMjd_D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize lengths of `Context` (Input)\n",
        "data['Context_Length'] = data['Context'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(data['Context_Length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Context Lengths')\n",
        "plt.xlabel('Length of Context')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x9kqbFe8ZtYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize lengths of `Response` (Output)\n",
        "data['Response_Length'] = data['Response'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(data['Response_Length'], bins=50, kde=True, color='teal')\n",
        "plt.title('Distribution of Response Lengths')\n",
        "plt.xlabel('Length of Response')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NxkjD_YKZzXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to split long sequences\n",
        "def split_long_sequences(sequence, max_length):\n",
        "    \"\"\"\n",
        "    Split a long sequence into chunks that fit within max_length.\n",
        "    \"\"\"\n",
        "    return [sequence[i:i + max_length] for i in range(0, len(sequence), max_length)]\n",
        "\n",
        "# Apply splitting to Context and Response\n",
        "data['Context_Chunks'] = data['Context'].apply(lambda x: split_long_sequences(x, max_seq_length))\n",
        "data['Response_Chunks'] = data['Response'].apply(lambda x: split_long_sequences(x, max_seq_length))\n",
        "\n",
        "# Flatten the dataset\n",
        "split_data = []\n",
        "for _, row in data.iterrows():\n",
        "    context_chunks = row['Context_Chunks']\n",
        "    response_chunks = row['Response_Chunks']\n",
        "    for context_chunk, response_chunk in zip(context_chunks, response_chunks):\n",
        "        split_data.append({'Spec_Text': context_chunk, 'Generated_Code': response_chunk})\n",
        "\n",
        "# Create a new DataFrame with the split data\n",
        "split_data_df = pd.DataFrame(split_data)\n",
        "\n",
        "# Convert to a HuggingFace dataset\n",
        "huggingface_dataset = Dataset.from_pandas(split_data_df)"
      ],
      "metadata": {
        "id": "LUNKcVg2gpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt template\n",
        "data_prompt = \"\"\"As a healthcare data analyst using SAS, generate SAS code based on the provided specification text. Use the same structure and logic as in the training examples, ensuring the code is ready for execution.\n",
        "\n",
        "### Specification:\n",
        "{}\n",
        "\n",
        "### SAS Code:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = \"<|endoftext|>\"  # Llama-specific EOS token\n",
        "\n",
        "# Format the prompt with input and response examples\n",
        "def formatting_prompt(examples):\n",
        "    inputs = examples[\"Spec_Text\"]\n",
        "    outputs = examples[\"Generated_Code\"]\n",
        "    texts = []\n",
        "    for input_, output in zip(inputs, outputs):\n",
        "        text = data_prompt.format(input_, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Format the dataset for training\n",
        "training_data = huggingface_dataset.map(formatting_prompt, batched=True)"
      ],
      "metadata": {
        "id": "8LknbZrRaMnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to W&B\n",
        "wandb.login()\n",
        "\n",
        "# Initialize W&B with synchronized configuration\n",
        "wandb.init(\n",
        "    project=\"final-fine-tuned-llm-sas\",\n",
        "    entity=\"cdrohan85-umbc\",\n",
        "    name=\"final-fine-tune-run\",\n",
        "    config={\n",
        "        \"model_name\": model_name,\n",
        "        \"max_seq_length\": max_seq_length,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"warmup_steps\": warmup_steps,\n",
        "        \"output_dir\": output_dir,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ALc3lkI7mBkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics\n",
        "accuracy_metric = load(\"accuracy\")\n",
        "bleu_metric = load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    valid_indices = labels != -100\n",
        "    predictions = predictions[valid_indices]\n",
        "    labels = labels[valid_indices]\n",
        "\n",
        "    # Decode predictions and references for BLEU\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu = bleu_metric.compute(\n",
        "        predictions=decoded_predictions,\n",
        "        references=[[ref] for ref in decoded_labels]\n",
        "    )\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"bleu\": bleu[\"bleu\"]}"
      ],
      "metadata": {
        "id": "EL8m0fIZiMpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoConfig\n",
        "\n",
        "def save_model_and_tokenizer(trained_model, tokenizer, output_dir):\n",
        "    # Define the target directory for saving all files\n",
        "    checkpoint_dir = os.path.join(output_dir, \"checkpoint-25\")\n",
        "\n",
        "    # Create the directory if it doesn't already exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save the Hugging Face-compatible config file (config.json)\n",
        "    config = AutoConfig.from_pretrained(model_name)  # Create a config.json\n",
        "    config.save_pretrained(checkpoint_dir)\n",
        "\n",
        "    # Save the model weights\n",
        "    trained_model.save_pretrained(checkpoint_dir)  # This will save both the PyTorch model (pytorch_model.bin)\n",
        "\n",
        "    # Save the model in safetensors format if necessary (for safety purposes)\n",
        "    trained_model.save_pretrained(checkpoint_dir, save_safetensors=True)  # To save model.safetensors\n",
        "\n",
        "    # Save the tokenizer files (tokenizer.json, tokenizer_config.json, etc.)\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "    print(f\"Model, tokenizer, and config files saved to {checkpoint_dir}\")\n"
      ],
      "metadata": {
        "id": "qECSxREWVAzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(training_data, model_name, max_seq_length, output_dir, learning_rate, num_epochs, batch_size):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True,\n",
        "        dtype=None,\n",
        "    )\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "        use_rslora=True,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=32,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=training_data,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=True,\n",
        "        args=TrainingArguments(\n",
        "            learning_rate=learning_rate,\n",
        "            lr_scheduler_type=lr_scheduler_type,  # Dynamic scheduler\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            num_train_epochs=num_epochs,\n",
        "            fp16=not is_bfloat16_supported(),  # Ensure only one precision type is enabled\n",
        "            bf16=is_bfloat16_supported(),\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            warmup_steps=warmup_steps,\n",
        "            output_dir=output_dir,\n",
        "            seed=0,\n",
        "        ),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the trained model and tokenizer\n",
        "    save_model_and_tokenizer(model, tokenizer, output_dir)"
      ],
      "metadata": {
        "id": "bFIlJLQagnf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training\n",
        "train_model(\n",
        "    training_data=training_data,\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "OpP535_I9TaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the model and tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "# Set up the model for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define the new specification text\n",
        "text = \"\"\"\n",
        "As a healthcare data analyst, please generate SAS code to identify persons with a condition and calculate estimates on use and expenditures for persons with the condition mental disorders (CCS Code = 650-670) in the year 2015. Use the same raw data inputs as in the original diabetes analysis and follow similar steps for identifying individuals, flagging, and calculating estimates.\n",
        "\"\"\"\n",
        "\n",
        "# Format the input for the model using the pre-defined prompt template\n",
        "formatted_prompt = data_prompt.format(\n",
        "    text,  # New specification\n",
        "    \"\"     # Leave the response empty for the model to fill in\n",
        ")\n",
        "\n",
        "# Prepare the input for the model\n",
        "inputs = tokenizer(\n",
        "    [formatted_prompt],               # Pass the formatted prompt\n",
        "    return_tensors=\"pt\"               # Generate PyTorch tensors\n",
        ").to(\"cuda\")                          # Send to GPU if available\n",
        "\n",
        "# Generate the output\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1500,              # Adjust to control output length\n",
        "    use_cache=True                    # Optimize for inference\n",
        ")\n",
        "\n",
        "# Decode and clean up the output\n",
        "answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "answer = answer.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Print the generated SAS code\n",
        "print(\"Generated SAS Code:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "jR48HMehr-od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"As a healthcare data analyst, please read in the permanent sas dataset CDATA.H190 to generate error-free SAS code that calculates and displays the use and expenditures for persons diagnosed with diabetes in 2016. Include all necessary data preparation steps, such as filtering for diabetes-related records, summarizing expenditures, and displaying the final results.\"\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    data_prompt.format(\n",
        "        #instructions\n",
        "        text,\n",
        "        #answer\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
        "answer=tokenizer.batch_decode(outputs)\n",
        "answer = answer[0].split(\"### Response:\")[-1]\n",
        "print(\"Answer of the question is:\", answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "duKnbRFCCSML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "liDcY8XOGFND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, Repository\n",
        "\n",
        "# Define your model directory and repository name\n",
        "model_dir = \"/content/drive/My Drive/School/UMBC/DATA690LLM/Results/Final/checkpoint-25\"\n",
        "repo_name = \"final-fine-tuned-sas-model\"  # Change if needed\n",
        "username = \"cwndrohan\"\n",
        "\n",
        "# Full repository path on Hugging Face\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Initialize and create the repository\n",
        "from huggingface_hub import create_repo\n",
        "create_repo(repo_id, exist_ok=True)\n",
        "\n",
        "# Upload the model files\n",
        "from huggingface_hub import upload_folder\n",
        "upload_folder(\n",
        "    folder_path=model_dir,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    ignore_patterns=[\"*.lock\"]  # Optional: Ignore unwanted files\n",
        ")\n"
      ],
      "metadata": {
        "id": "EScOXRUEZnzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the model card\n",
        "\n",
        "# Define the repository\n",
        "repo_id = \"cwndrohan/final-fine-tuned-sas-model\"  # Repository name\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Upload the README.md file\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/drive/My Drive/School/UMBC/DATA690LLM/README.md\",  # Path to the README.md file\n",
        "    path_in_repo=\"README.md\",             # Destination path in the repository\n",
        "    repo_id=repo_id,                      # Model repository ID\n",
        "    commit_message=\"Adding model card\"    # Commit message\n",
        ")\n",
        "\n",
        "print(f\"Model card added to: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "TPUVtZRiI7QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the repo\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Define repository details\n",
        "repo_name = \"final-fine-tuned-sas-model\"\n",
        "username = \"cwndrohan\"\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Create the repository\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, private=False)\n",
        "\n",
        "print(f\"Repository created: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "QLTpl44fHNlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push model to HF\n",
        "\n",
        "# Define paths and model details\n",
        "local_path = \"/content/drive/My Drive/School/UMBC/DATA690LLM/Results/Final/checkpoint-25\"\n",
        "repo_name = \"final-fine-tuned-sas-model\"  # Replace with your preferred model name\n",
        "username = \"cwndrohan\"                    # Hugging Face username\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Push the model to the Hub\n",
        "api.upload_folder(\n",
        "    folder_path=local_path,\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Uploading fine-tuned SAS code generator model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "sWUTgJUrG8ug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}